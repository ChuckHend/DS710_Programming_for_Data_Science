{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS700 Final Project\n",
    "#### Analysis of Optum vs. UHC public sentiment via twitter tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read credential file\n",
    "creds= pd.read_csv('credentials.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['consumerKey', 'consumerSecret', 'tokenKey', 'tokenSecret'], dtype='object')\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "# inspect ... \n",
    "print(creds.columns)\n",
    "print(creds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign credentials\n",
    "con_key = creds.consumerKey[0]\n",
    "con_secret = creds.consumerSecret[0]\n",
    "acc_token = creds.tokenKey[0]\n",
    "acc_secret = creds.tokenSecret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, \n",
    "                           consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adam Hendel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm user\n",
    "api.me().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_tweets(terms, num_needed=2000):\n",
    "    '''Accepts a list of terms to search twitter. Does not return a unique list of tweets\\\n",
    "    can be overlap in tweets across terms. eg one tweet could contain both terms=['texas', 'tx']'''\n",
    "    tweets = []\n",
    "    last_id = -1 # id of last tweet seen\n",
    "    progress = 0\n",
    "    for term in terms:     \n",
    "        tweet_more = True\n",
    "        while tweet_more: # while still finding tweets under a term\n",
    "                if len(tweets) >= num_needed: # if we reached or gather req., exit the function\n",
    "                    print('Reached  Limit: {}'.format(len(tweets)))\n",
    "                    return tweets\n",
    "                \n",
    "                try: # try to get more\n",
    "                    query = api.search(q = term, count = 100, max_id = str(last_id - 1), lang='en')\n",
    "                    print('{} : {} tweets'.format(term, len(query)))\n",
    "                except tweepy.TweepError as e:\n",
    "                    print(\"Error\", e)\n",
    "                    break\n",
    "                else:\n",
    "                    if query: # if we found more, extend them to our tweet list\n",
    "                        tweets.extend(query)\n",
    "                        last_id = tweets[-1].id # take note of the tweet id\n",
    "                    else: # move on to next term\n",
    "                        print('No more tweets under \"{}\"'.format(term))\n",
    "                        tweet_more = False # stop searching this term\n",
    "                        last_id = -1 # reset the tweet id for this term\n",
    "                        break\n",
    "\n",
    "    tot_tweets = len(tweets)\n",
    "    if tot_tweets < num_needed:\n",
    "        print('Exhausted Search: Found {} Tweets. . .'.format(tot_tweets))\n",
    "    return tweets # if we've exhausted our search terms, return what we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build list of search terms\n",
    "optum_terms = ['#%23optum', '@optum', 'optum']\n",
    "uhc_terms = ['#%uhc', '#%myuhc', '@AskUHC', 'unitedhealthcare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#%23optum : 24 tweets\n",
      "#%23optum : 0 tweets\n",
      "No more tweets under \"#%23optum\"\n",
      "@optum : 100 tweets\n",
      "@optum : 100 tweets\n",
      "@optum : 26 tweets\n",
      "@optum : 0 tweets\n",
      "No more tweets under \"@optum\"\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 89 tweets\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 96 tweets\n",
      "optum : 100 tweets\n",
      "optum : 100 tweets\n",
      "optum : 80 tweets\n",
      "optum : 0 tweets\n",
      "No more tweets under \"optum\"\n",
      "Exhausted Search: Found 1515 Tweets. . .\n"
     ]
    }
   ],
   "source": [
    "# get optum tweets\n",
    "optumTweets = gather_tweets(optum_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "#%uhc : 100 tweets\n",
      "Reached  Limit: 2000\n"
     ]
    }
   ],
   "source": [
    "# get uhc tweets\n",
    "uhcTweets = gather_tweets(uhc_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tweet objects for later\n",
    "with open('optum_tweets.pkl', 'wb') as output:\n",
    "    pickle.dump(optumTweets, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open('uhc_tweets.pkl', 'wb') as output:\n",
    "    pickle.dump(uhcTweets, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open raw tweet file\n",
    "with open('optum_tweets.pkl', 'rb') as input:\n",
    "    optum = pickle.load(input)\n",
    "with open('uhc_tweets.pkl', 'rb') as input:\n",
    "    uhc = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.00261437908496732\n"
     ]
    }
   ],
   "source": [
    "# determine number of tweets contain geolocation\n",
    "geos = 0\n",
    "for tweet in optum:\n",
    "    if tweet.geo is not None:\n",
    "        geos = geos + 1\n",
    "print(geos, geos/len(optum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    }
   ],
   "source": [
    "geos = 0\n",
    "for tweet in uhc:\n",
    "    if tweet.geo is not None:\n",
    "        geos = geos + 1\n",
    "print(geos, geos/len(uhc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## very sparse geo data---excluding from analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_emoji(tweets, emoji=':)'):\n",
    "    '''counts number of emojis in a list of tweets'''\n",
    "    tot_emoji = 0\n",
    "    for tweet in tweets:\n",
    "        if ':)' in tweet.text:\n",
    "            tot_emoji = tot_emoji + 1\n",
    "    return tot_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_char(tweets):\n",
    "    '''drops list of characters'''\n",
    "    dropChar='.\"#$%&\\'()*+,-/:;<=>@[\\\\]^_`{|}~â€¦'\n",
    "    cleaned = []\n",
    "    for tweet in tweets:\n",
    "        clean = ''.join([c for c in tweet if c not in dropChar])\n",
    "        cleaned.append(clean)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_scrub(tweets):\n",
    "    '''removes urls, hashtags and usernames, and retweet annotations'''\n",
    "    cleaned = []\n",
    "    for tweet in tweets:\n",
    "        # drop urls, hastags, @targets\n",
    "        clean_tweet = re.sub(r\"(http|www|#|@|RT )\\S+\", \"\", tweet)\n",
    "        clean_tweet = re.sub(r'[^\\x00-\\x7F]+',' ', clean_tweet)\n",
    "        # drop extra white space\n",
    "        clean_tweet = ' '.join(clean_tweet.split())\n",
    "        if len(clean_tweet) > 1: # remove tweets that are 0 or 1 char long\n",
    "            cleaned.append(clean_tweet)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def screen_org(tweets, orgList):\n",
    "    '''removes any tweets that are within the orgList'''\n",
    "    tweet_list = []\n",
    "    for tweet in tweets:\n",
    "        if not np.any([x.upper() in tweet.user.name.upper() for x in orgList]):\n",
    "            # skip this tweet if any match of text from orgList to username\n",
    "            # we dont want tweets from the organizations\n",
    "            tweet_list.append(tweet.text)\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweets(tweets, orgList):\n",
    "    '''clean and convert each tweet object into a list of tweets'''\n",
    "    # remove tweets from the originating organization\n",
    "    tweet_list = screen_org(tweets, orgList)\n",
    "    \n",
    "    # remove duplicates (note this step is first to prevent removing retweets)\n",
    "    tweet_list = list(set(tweet_list))\n",
    "    \n",
    "    # clean up the texts\n",
    "    tweets_clean = tweet_scrub(tweet_list)\n",
    "    \n",
    "    # drop special characters\n",
    "    tweets_clean = drop_char(tweets_clean)\n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoji_df = pd.DataFrame({'Optum':[count_emoji(optum), len(optum)],\n",
    "                         'UHC':[count_emoji(uhc), len(uhc)]},\n",
    "                        index=['num_emojis', 'num_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Optum   UHC\n",
      "num_emojis      0     9\n",
      "num_tweets   1530  2000\n"
     ]
    }
   ],
   "source": [
    "# save results to file\n",
    "print(emoji_df)\n",
    "emoji_df.to_csv('emoji_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up the tweet objects\n",
    "uhg = ['optum', 'uhc', 'uhg', 'unitedhealth']\n",
    "optum_clean = process_tweets(tweets=optum, orgList=uhg)\n",
    "uhc_clean = process_tweets(tweets=uhc, orgList=uhg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942\n",
      "1225\n"
     ]
    }
   ],
   "source": [
    "# how many tweets are we left with after cleaning\n",
    "print(len(optum_clean))\n",
    "print(len(uhc_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save cleaned to dataframe\n",
    "minLen = min(len(optum_clean), len(uhc_clean))\n",
    "tweet_df = pd.DataFrame({'optumTweets':optum_clean[:minLen],\n",
    "                         'uhcTweets':uhc_clean[:minLen]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_df.to_csv('tweets_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['optumTweets', 'uhcTweets'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read clean tweets in\n",
    "tweet_df=pd.read_csv('tweets_cleaned.csv')\n",
    "tweet_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conduct sentiment on each tweet, save results\n",
    "for col in tweet_df.columns:\n",
    "    tweets = tweet_df[col]\n",
    "    blob = tweets.apply(TextBlob)\n",
    "    pol = []\n",
    "    sub = []\n",
    "    for b in blob:\n",
    "        pol.append(b.sentiment.polarity)\n",
    "        sub.append(b.sentiment.subjectivity)\n",
    "    tweet_df['{}_pol'.format(col)] = pol\n",
    "    tweet_df['{}_sub'.format(col)] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optumTweets</th>\n",
       "      <th>uhcTweets</th>\n",
       "      <th>optumTweets_pol</th>\n",
       "      <th>optumTweets_sub</th>\n",
       "      <th>uhcTweets_pol</th>\n",
       "      <th>uhcTweets_sub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>SHYFT Analytics is another garbage Optum VC co...</td>\n",
       "      <td>UHC every day at 512557623940038</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Do you ever forget important questions to ask ...</td>\n",
       "      <td>Absolutely  the key to tackling malaria and HI...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>unhoptum venture fund is a fantastic idea</td>\n",
       "      <td>20 para 400 waaa v UHC BADLION FFA  Road To 40...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Does your get diaper rash? What remedies work ...</td>\n",
       "      <td>The donut hole should be against the law They ...</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Your baby will a lot in their first year Find ...</td>\n",
       "      <td>Nick carpigaming</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Look at NHS supply chain CSUs OptumUH Big4 manag</td>\n",
       "      <td>All people have the right to health no matter ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.267857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Employee Wellnes Is A Culture At Optum Gayatri...</td>\n",
       "      <td>FFA pls</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Optum Ventures launched will invest 250 millio...</td>\n",
       "      <td>Uhc for 4k pls</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>UNH after announcement of 250M fund to invest ...</td>\n",
       "      <td>why does everyone on Hypixel Solo UHC !! play ...</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>ICYMI Through new Optum Ventures unit UnitedHe...</td>\n",
       "      <td>I liked a video GER UHC Bedwars SkyWars und mehr</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>NYT Insurers are looking to follow the strateg...</td>\n",
       "      <td>oh gott xd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>launches 250M fund to invest in startups whos ...</td>\n",
       "      <td>UHC2 FFA Timber TripleOres BackPacks CutClean ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Through new unit will invest 250M in</td>\n",
       "      <td>Dont miss sneak peak of my session at with Reb...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Optum Announces 250 Million Fund to Invest in ...</td>\n",
       "      <td>! UHC NORMAL To6 ! Test Core 19132 12X Win10  ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Well again strives to underachieve For 4 month...</td>\n",
       "      <td>Sir ifl properly executed the advantages are e...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           optumTweets  \\\n",
       "231  SHYFT Analytics is another garbage Optum VC co...   \n",
       "692  Do you ever forget important questions to ask ...   \n",
       "214          unhoptum venture fund is a fantastic idea   \n",
       "52   Does your get diaper rash? What remedies work ...   \n",
       "906  Your baby will a lot in their first year Find ...   \n",
       "267   Look at NHS supply chain CSUs OptumUH Big4 manag   \n",
       "85   Employee Wellnes Is A Culture At Optum Gayatri...   \n",
       "874  Optum Ventures launched will invest 250 millio...   \n",
       "358  UNH after announcement of 250M fund to invest ...   \n",
       "860  ICYMI Through new Optum Ventures unit UnitedHe...   \n",
       "322  NYT Insurers are looking to follow the strateg...   \n",
       "930  launches 250M fund to invest in startups whos ...   \n",
       "446               Through new unit will invest 250M in   \n",
       "866  Optum Announces 250 Million Fund to Invest in ...   \n",
       "350  Well again strives to underachieve For 4 month...   \n",
       "\n",
       "                                             uhcTweets  optumTweets_pol  \\\n",
       "231                  UHC every day at 512557623940038          0.500000   \n",
       "692  Absolutely  the key to tackling malaria and HI...         0.450000   \n",
       "214  20 para 400 waaa v UHC BADLION FFA  Road To 40...         0.400000   \n",
       "52   The donut hole should be against the law They ...        -0.187500   \n",
       "906                                   Nick carpigaming         0.375000   \n",
       "267  All people have the right to health no matter ...         0.000000   \n",
       "85                                             FFA pls         0.000000   \n",
       "874                                     Uhc for 4k pls         0.000000   \n",
       "358  why does everyone on Hypixel Solo UHC !! play ...        -0.050000   \n",
       "860   I liked a video GER UHC Bedwars SkyWars und mehr         0.068182   \n",
       "322                                         oh gott xd         0.000000   \n",
       "930  UHC2 FFA Timber TripleOres BackPacks CutClean ...         0.000000   \n",
       "446  Dont miss sneak peak of my session at with Reb...         0.136364   \n",
       "866  ! UHC NORMAL To6 ! Test Core 19132 12X Win10  ...         0.000000   \n",
       "350  Sir ifl properly executed the advantages are e...         0.000000   \n",
       "\n",
       "     optumTweets_sub  uhcTweets_pol  uhcTweets_sub  \n",
       "231         0.500000       0.000000       0.000000  \n",
       "692         0.750000       0.050000       0.516667  \n",
       "214         0.900000       0.000000       0.000000  \n",
       "52          0.500000       0.000000       0.000000  \n",
       "906         0.416667       0.000000       0.000000  \n",
       "267         0.000000       0.142857       0.267857  \n",
       "85          0.000000       0.000000       0.000000  \n",
       "874         0.000000       0.000000       0.000000  \n",
       "358         0.200000       0.000000       0.000000  \n",
       "860         0.227273       0.600000       0.800000  \n",
       "322         0.000000       0.000000       0.000000  \n",
       "930         0.000000       0.000000       0.000000  \n",
       "446         0.454545       0.000000       0.000000  \n",
       "866         0.000000       0.187500       0.650000  \n",
       "350         0.357143       0.000000       0.666667  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data\n",
    "tweet_df.sample(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['optumTweets_pol', 'optumTweets_sub', 'uhcTweets_pol', 'uhcTweets_sub'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# save to file: for analysis in R\n",
    "print(tweet_df.columns[2:])\n",
    "tweet_df[tweet_df.columns[2:]].to_csv('optum_uhc_sentiments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
